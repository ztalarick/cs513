predValid <- predict(model2, test, type = "class")
mean(predValid == ValidSet$Condition)
mean(predValid == test$Class)
table(predValid, test$Class)
#6.2 Random Forrest Classification Model
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw6")
data <- read.csv("breast-cancer-wisconsin.data.csv")
#fix data
data2 <- na.omit(data)
#split into test and training data
index<-sort(sample(nrow(data2),round(.25*nrow(data2))))
training<-data2[-index,]
test<-data2[index,]
#random forrest package
#install.packages("randomForest")
library(randomForest)
model1 <- randomForest(Class ~ ., data = training, importance = TRUE)
str(model1)
model2 <- randomForest(Class ~ ., data = training, ntree = 500, mtry = 6, importance = TRUE)
str(model2)
predTrain <- predict(model1, training, type = "class")
table(predTrain, training$Class)
predValid <- predict(model1, test, type = "class")
mean(predValid == test$Class)
table(predValid, test$Class)
install.packages(c("neuralnet", "tidyverse"))
#library for neural network
library(neuralnet)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw6")
data <- read.csv("breast-cancer-wisconsin.data.csv")
attach(data)
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
maxmindf <- as.data.frame(lapply(data, normalize))
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw6")
data <- read.csv("breast-cancer-wisconsin.data.csv")
attach(data)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw6")
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw6")
data <- read.csv("wisc_bc_ContinuousVar.csv")
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
attach(data)
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
maxmindf <- as.data.frame(lapply(data, normalize))
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
attach(data)
#normalize data
#Can use scaled normalizaton
scaled_data<-scale(data)
View(data)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
attach(data)
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
maxmindf <- as.data.frame(lapply(data, normalize))
maxmindf <- as.data.frame(lapply(data[-c(1, 2)], normalize))
maxmindf$diagnosis=data[2]
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#Neural Network
library(neuralnet)
nn <- neuralnet(Class ~ , data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
nn <- neuralnet(Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +
compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean
, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data$diagnosis=as.numeric(data[2])
#remove ID and Diagnosis to normalize the numeric data
maxmindf <- as.data.frame(lapply(data[-c(1)], normalize))
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data$diagnosis=as.numeric(unlist(data[2]))
#remove ID and Diagnosis to normalize the numeric data
maxmindf <- as.data.frame(lapply(data[-c(1)], normalize))
#add Diagnosis back in
maxmindf$diagnosis=data[2]
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#Neural Network
library(neuralnet)
nn <- neuralnet(Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +
compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean
, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
View(data)
View(data)
View(data)
View(maxmindf)
View(maxmindf)
nn <- neuralnet(Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +
compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se
+ smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst
+ smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
#Neural Network
library(neuralnet)
nn <- neuralnet(Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +
compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se
+ smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst
+ smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
, data=train, hidden=c(5,1)=train, linear.output=FALSE, threshold=0.01)
#Neural Network
library(neuralnet)
nn <- neuralnet(Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +
compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se
+ smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst
+ smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
nn <- neuralnet(Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +
compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se
+ smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst
+ smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
nn$result.matrix
plot(nn)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data$diagnosis=as.numeric(unlist(data[2]))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data[-c(1)], normalize))
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
f <- Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +
compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se
+ smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst
+ smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
data2 <- na.omit(data)
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data2$diagnosis=as.numeric(unlist(data2[2]))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data2[-c(1)], normalize))
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
f <- Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#get red of all rows with a 0
data2 <- row_sub = apply(data, 1, function(row) all(row !=0 ))
#get red of all rows with a 0
data2 <- apply(data, 1, function(row) all(row !=0 ))
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data2$diagnosis=as.numeric(unlist(data2[2]))
View(data2)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#get red of all rows with a 0
row_sub = apply(data, 1, function(row) all(row !=0 ))
data[row_sub,]
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#get red of all rows with a 0
row_sub = apply(data, 1, function(row) all(row !=0 ))
data2 <- data[row_sub,]
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data2$diagnosis=as.numeric(unlist(data2[2]))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data2[-c(1)], normalize))
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
f <- Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
nn$result.matrix
plot(nn)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#get red of all rows with a 0
#row_sub = apply(data, 1, function(row) all(row !=0 ))
#data2 <- data[row_sub,]
data2 <- data
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data2$diagnosis=as.numeric(unlist(data2[2]))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data2[-c(1)], normalize))
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
f <- Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#get red of all rows with a 0
row_sub = apply(data, 1, function(row) all(row !=0 ))
data3 <- data[row_sub,]
data2 <- na.omit(data3)
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data2$diagnosis=as.numeric(unlist(data2[2]))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data2[-c(1)], normalize))
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
f <- Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
#formula for NN
#f <- Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
f <- Class ~ radius_mean + texture_mean
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
setwd("/home/zach/Documents/Junior/CS513/hw7")
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data2[-c(1, 2)], normalize))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data[-c(1, 2)], normalize))
View(maxmindf)
View(maxmindf)
maxmindf$diagnosis = data[2]
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
#f <- Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
f <- Class ~ radius_mean + texture_mean
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
nn$result.matrix
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#get red of all rows with a 0
#row_sub = apply(data, 1, function(row) all(row !=0 ))
#data3 <- data[row_sub,]
#data2 <- na.omit(data3)
attach(data)
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data[-c(1, 2)], normalize))
maxmindf$diagnosis = data[2]
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
#f <- Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
f <- Class ~ radius_mean + texture_mean
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data$diagnosis=as.numeric(unlist(data[2]))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data[-c(1)], normalize))
View(data)
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
#f <- Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
f <- Class ~ radius_mean + texture_mean
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
View(maxmindf)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data$diagnosis=as.numeric(unlist(data[2]))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data[-c(1)], normalize))
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
#f <- Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
f <- Class ~ radius_mean + texture_mean
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data$diagnosis=as.numeric(unlist(data[2]))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data[-c(1)], normalize))
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
#f <- Class ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
f <- Class ~ radius_mean
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
nn$result.matrix
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data$diagnosis=as.numeric(unlist(data[2]))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data[-c(1)], normalize))
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
f <- Diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data$diagnosis=as.numeric(unlist(data[2]))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data[-c(1)], normalize))
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
f <- diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
nn$result.matrix
plot(nn)
#Test the resulting output
temp_test <- subset(testset, select = c("fcfps","earnings_growth", "de", "mcap", "current_ratio"))
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw7")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#max min normilzation
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#make diagnosis numeric, 1 = B and 2 = M
data$diagnosis=as.numeric(unlist(data[2]))
#remove ID to normalize the numeric data
maxmindf <- as.data.frame(lapply(data[-c(1)], normalize))
#split data into 80% training and 20% testing
# 1-456 for training and 456-570 for testing
train <- maxmindf[1:456, ]
test <- maxmindf[457:570, ]
#formula for NN
f <- diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean+ smoothness_mean +  compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave.points_se + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concave.points_worst+ concavity_worst + symmetry_worst + fractal_dimension_worst
#Neural Network
library(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,1), linear.output=FALSE, threshold=0.01)
nn$result.matrix
plot(nn)
#Test the resulting output
temp_test <- subset(test, select = c("fcfps","earnings_growth", "de", "mcap", "current_ratio"))
head(temp_test)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw8")
data <- read.csv("wisc_bc_ContinuousVar.csv")
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw8")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#scale data
scaled_data<-scale(data)
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw8")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#scale data
#make diagnosis numeric, 1 = B and 2 = M
data$diagnosis=as.numeric(unlist(data[2]))
scaled_data<-scale(data[c(-1)])
library(cluster)
distance <- get_dist(sca)
install.packages("factoextra")
rm(list=ls())
setwd("/home/zach/Documents/Junior/CS513/hw8")
data <- read.csv("wisc_bc_ContinuousVar.csv")
#scale data
#make diagnosis numeric, 1 = B and 2 = M
data$diagnosis=as.numeric(unlist(data[2]))
scaled_data<-scale(data[c(-1)])
library(factoextra)
devtools::install_github("kassambara/ggpubr")
install.packages("rlang",type="win.binary")
library(readr)
library(ggplot2)
setwd("/home/zach/Documents/Junior/CS513/hw7")
data = read_csv("wisc_bc_ContinuousVar.csv")
data = data.frame(data)
